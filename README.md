# Neural-Style-transfer
This paper reviews neural style transfer applications from several aspects. This paper asks and answers questions such as how does change in parameters of representations based on a pre-trained convolutional neural network affect the visual results in synthesized images. Three content images and three different style images selected to generate nine blended images for comparison. The 19-layer VGG network model serves as the primary framework for this analysis. Modifications to the original implementation such as adjusting learning rates, exploring different initializing image, changing training iterations, experimenting different random seeds, and attempting relative weights of content and style loss in the loss function help observe the blended image difference, etc. The results from trial and error will be discussed and explained to reach conclusion and further research suggestions.
